{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Naive-Bayes-NLP.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMbhXtK/tugzZuxcNernUqT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JonNData/naive_bayes/blob/master/notebooks/Naive_Bayes_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xnU1sodYIu8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwG6BK2CokiI",
        "colab_type": "text"
      },
      "source": [
        "## Multinomial Naive Bayes\n",
        "Input X: array of messages  \n",
        "Input y: array of labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtocMzZxYTL9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MNNaiveBayes:\n",
        "  def __init__(self, k=0.5):\n",
        "    self.k = k\n",
        "    self.word_probs = []\n",
        "\n",
        "  def tokenize(self, document):\n",
        "    \"\"\"\n",
        "    Take in a document and return a list of words\n",
        "    \"\"\"\n",
        "    doc = document.lower()\n",
        "    # remove non-alpha characters\n",
        "    stop_chars = '''0123456789!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
        "    \n",
        "    tokens = \"\"\n",
        "    # iterate through and make each token\n",
        "    for char in doc:\n",
        "      if char not in stop_chars:\n",
        "        tokens += char\n",
        "\n",
        "    return tokens.split() # now a list of tokens\n",
        "  \n",
        "  def count_words(self, X, y):\n",
        "    \"\"\"\n",
        "    X is an array of documents\n",
        "    y is an array of targets, 0 or 1\n",
        "    Output a dictionary of {word: (cat0_count, cat1_count)...}\n",
        "    \"\"\"\n",
        "    counts = {}\n",
        "    for document in X:\n",
        "      for category in y:\n",
        "        for token in self.tokenize(document):\n",
        "          # Initialize a dict entry with 0 counts\n",
        "          if token not in counts:\n",
        "            counts[token] = [0,0]\n",
        "          # Now that it exists, add to the category count for that word\n",
        "          counts[token][category] += 1\n",
        "    return counts\n",
        "\n",
        "  def prior_prob(self, counts):\n",
        "    \n",
        "    # Iterate through counts dict and add up each word count by category\n",
        "    cat0_word_count = cat1_word_count = 0\n",
        "    for word, (cat0_count, cat1_count) in counts.items():\n",
        "        cat0_word_count += cat0_count\n",
        "        cat1_word_count += cat1_count\n",
        "    \n",
        "    # Get the prior prob by dividing words in each cat by total words\n",
        "    cat_0_prior = cat0_word_count / (cat0_word_count + cat1_word_count)\n",
        "    cat_1_prior = cat1_word_count / (cat0_word_count + cat1_word_count)\n",
        "    return cat_0_prior, cat_1_prior\n",
        "\n",
        "  def word_probabilities()\n",
        "    \"\"\"\n",
        "    Output: \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}