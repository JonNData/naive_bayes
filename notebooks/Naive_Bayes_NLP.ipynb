{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Naive-Bayes-NLP.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMTllAEbzkMZqwJkQNg85f7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JonNData/naive_bayes/blob/master/notebooks/Naive_Bayes_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xnU1sodYIu8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwG6BK2CokiI",
        "colab_type": "text"
      },
      "source": [
        "## Multinomial Naive Bayes\n",
        "Input X: array of messages  \n",
        "Input y: array of labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtocMzZxYTL9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MNNaiveBayes:\n",
        "  def __init__(self, k=0.5):\n",
        "    self.k = k\n",
        "    self.cat0_count = 0\n",
        "    self.cat1_count = 0\n",
        "    self.total_count = self.cat0_count + self.cat1_count\n",
        "    self.cat_0_prior = 0\n",
        "    self.cat_1_prior = 0\n",
        "    self.cat_0_prior, self.cat_1_prior\n",
        "    self.word_probs = []\n",
        "    self.vocab = []\n",
        "\n",
        "  def tokenize(self, document):\n",
        "    \"\"\"\n",
        "    Take in a document and return a list of words\n",
        "    \"\"\"\n",
        "    doc = document.lower()\n",
        "    # remove non-alpha characters\n",
        "    stop_chars = '''0123456789!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
        "    \n",
        "    tokens = \"\"\n",
        "    # iterate through and make each token\n",
        "    for char in doc:\n",
        "      if char not in stop_chars:\n",
        "        tokens += char\n",
        "\n",
        "    return tokens.split() # now a list of tokens\n",
        "  \n",
        "  def count_words(self, X, y):\n",
        "    \"\"\"\n",
        "    X is an array of documents\n",
        "    y is an array of targets, 0 or 1\n",
        "    Output a dictionary of {word: (cat0_count, cat1_count)...}\n",
        "    \"\"\"\n",
        "    counts = {}\n",
        "    for document in X:\n",
        "      for category in y:\n",
        "        for token in self.tokenize(document):\n",
        "          # Initialize a dict entry with 0 counts\n",
        "          if token not in counts:\n",
        "            counts[token] = [0,0]\n",
        "          # Now that it exists, add to the category count for that word\n",
        "          counts[token][category] += 1\n",
        "    return counts\n",
        "\n",
        "  def prior_prob(self, counts):\n",
        "    \n",
        "    # Iterate through counts dict and add up each word count by category\n",
        "    cat0_word_count = cat1_word_count = 0\n",
        "    for word, (cat0_count, cat1_count) in counts.items():\n",
        "        cat0_word_count += cat0_count\n",
        "        cat1_word_count += cat1_count\n",
        "\n",
        "    # save attributes to the class\n",
        "    self.cat0_count = cat0_word_count\n",
        "    self.cat1_count = cat1_word_count\n",
        "    self.total_count = self.cat0_count + self.cat1_count\n",
        "\n",
        "    # Get the prior prob by dividing words in each cat by total words\n",
        "    cat_0_prior = cat0_word_count / self.total_count\n",
        "    cat_1_prior = cat1_word_count / self.total_count\n",
        "    return cat_0_prior, cat_1_prior\n",
        "\n",
        "  def word_probabilities(self, counts):\n",
        "    \"\"\"turn the word_counts into a list of triplets\n",
        "    word, p(w | cat0), and p(w | cat1)\"\"\"\n",
        "    # Here we apply the smoothing term, self.k, so that words that aren't in\n",
        "    # the category don't get calculated as 0\n",
        "    self.vocab = [word for word, (cat1, cat2) in count.items()]\n",
        "    return [(word,\n",
        "    (cat0 + self.k) / (self.cat0_count + 2 * self.k),\n",
        "    (cat1 + self.k) / (self.cat1_count + 2 * self.k))\n",
        "    for word, (cat1, cat2) in counts.items()]\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    # Take all these functions and establish probabilities of input\n",
        "    counts = self.count_words(X, y)\n",
        "    self.cat_0_prior, self.cat_1_prior = self.prior_prob(counts)\n",
        "    self.word_probs = self.word_probabilities(counts)\n",
        "\n",
        "  def predict(self, test_corpus):\n",
        "    # Split the text into tokens,\n",
        "    # For each category: calculate the probability of each word in that cat\n",
        "    # find the product of all of them and the prior prob of that cat\n",
        "    y_pred = []\n",
        "    log_prob_cat0 = log_prob_cat1 = 0.0\n",
        "    for document in test_corpus:\n",
        "      tokens = self.tokenize(text)\n",
        "      for token in tokens:\n",
        "        # if the token is in the vocab add all the log prob to be exp later\n",
        "        # if not don't do anything. We just need a score for each category/doc\n",
        "        if token in self.vocab\n",
        "        \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7Ofhq05jzSU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d316e459-62be-4651-88f1-72ac5389900a"
      },
      "source": [
        "tees =[[\"asdf\", 0.1, 0.2], [\"qwer\", 0.4, 0.6], [\"tryu\", 0.3, 0.8]]\n",
        "[item[0] for item in tees]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['asdf', 'qwer', 'tryu']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    }
  ]
}