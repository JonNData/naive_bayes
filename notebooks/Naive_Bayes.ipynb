{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Naive-Bayes.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN8IyMqpYERacqkU+tK2GkY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JonNData/naive_bayes/blob/master/notebooks/Naive_Bayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6Lb44tkJVbs",
        "colab_type": "text"
      },
      "source": [
        "## Naive Bayes\n",
        "Multinomial \n",
        "Need a class.  \n",
        "Fit and predict methods.  \n",
        "\n",
        "\n",
        "*   Fit will get the histogram frequency, and train the model, update attributes.\n",
        "-- Need a feature vector, row is obs, columns are features, numerical\n",
        "*   Predict will classify the observation  \n",
        "\n",
        "  \n",
        "\n",
        "What's the input? Needs to be categorical but that can be easily arranged.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4UMwJs63Pq1",
        "colab_type": "text"
      },
      "source": [
        "## Quick Example with Balance Scale data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JU2oSj5ZJUjv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyGcA8zgOQKo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "headers = ['class_name', 'left-weight', 'left-distance', 'right-weight', 'right-distance']"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBaDn8EPN-qy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/balance-scale/balance-scale.data', names=headers)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-OJBZwDOruo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train, test = train_test_split(df, test_size = 0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tplK6SW3PVQv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = train.drop(columns='class_name')\n",
        "X_test = test.drop(columns='class_name')\n",
        "\n",
        "y_train = train['class_name'].apply(lambda x: 0 if x==\"L\" else 1) \n",
        "y_test = test['class_name'].apply(lambda x: 0 if x==\"L\" else 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tx4THrnahy6x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train.shape, X_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjDHLVOy2yfb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnb = MultinomialNB()\n",
        "mnb.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yi_Fg-qn297X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = mnb.predict(X_test)\n",
        "accuracy_score(y_test, y_pred=y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItRielG55D-8",
        "colab_type": "text"
      },
      "source": [
        "## Now to make the class\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYBWra0n5Ce7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksB4dySl69L5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GaussBayes:\n",
        "  def __init__(self):\n",
        "    pass\n",
        "  def fit(self, X_train, y_train):\n",
        "    # Get information about the array\n",
        "    num_obs, num_features = X_train.shape\n",
        "    \n",
        "    # Get array of unique targets\n",
        "    self.u_classes = np.unique(y_train)\n",
        "    num_classes = len(self.u_classes)\n",
        "\n",
        "    # intialize mean, variance, priors with correct size\n",
        "    self.X_train_mean = np.zeros((num_classes, num_features), dtype=np.float64)\n",
        "    self.var = np.zeros((num_classes, num_features), dtype=np.float64)\n",
        "    self.priors = np.zeros(num_classes, dtype=np.float64)\n",
        "\n",
        "    # Calculate\n",
        "    for i,cls in enumerate(self.u_classes):\n",
        "      # specify only values where the class is the same as the target\n",
        "      X_train_cls = X_train[cls==y_train]\n",
        "      # the mean from this class and all columns\n",
        "      self.X_train_mean[cls,:] = X_train_cls.mean(axis=0)\n",
        "      self.var[cls,:] = X_train_cls.var(axis=0)\n",
        "      self.priors[cls] = X_train_cls.shape[0] / float(num_obs) # how often this class is occuring\n",
        "\n",
        "  def predict(self, X_test):\n",
        "    y_pred = [self._predict(x) for x in X_test]\n",
        "    return y_pred\n",
        "\n",
        "  def _predict(self, X_test):\n",
        "    posteriors =  []\n",
        "\n",
        "    for i, cls in enumerate(self.u_classes):\n",
        "      prior = np.log(self.priors[i])\n",
        "      class_conditional = np.sum(np.log(self._prob_density(i, X_test)))\n",
        "      posterior =  prior + class_conditional\n",
        "      posteriors.append(posterior)\n",
        "    \n",
        "    return self.u_classes[np.argmax(posteriors)] # argmax gives index of max value\n",
        "\n",
        "  def _prob_density(self, class_idx, x):\n",
        "      mean = self.X_train_mean[class_idx]\n",
        "      var = self.var[class_idx]\n",
        "      numerator = np.exp(- (x-mean)**2 / (2 * var))\n",
        "      denominator = np.sqrt(2 * np.pi * var)\n",
        "      return numerator / denominator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uztS9RaABxNG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gb = GaussBayes()\n",
        "gb.fit(X_train, y_train)\n",
        "y_pred = gb.predict(X_test)\n",
        "accuracy_score(y_test, y_pred=y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfNNXTD4Cavj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NaiveBayes:\n",
        "    def __init__(self, X, y):\n",
        "        self.num_examples, self.num_features = X.shape\n",
        "        self.num_classes = len(np.unique(y))\n",
        "        self.eps = 1e-6\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.classes_mean = {}\n",
        "        self.classes_variance = {}\n",
        "        self.classes_prior = {}\n",
        "\n",
        "        for c in range(self.num_classes):\n",
        "            X_c = X[y == c]\n",
        "\n",
        "            self.classes_mean[str(c)] = np.mean(X_c, axis=0)\n",
        "            self.classes_variance[str(c)] = np.var(X_c, axis=0)\n",
        "            self.classes_prior[str(c)] = X_c.shape[0] / X.shape[0]\n",
        "\n",
        "    def predict(self, X):\n",
        "        probs = np.zeros((self.num_examples, self.num_classes))\n",
        "\n",
        "        for c in range(self.num_classes):\n",
        "            prior = self.classes_prior[str(c)]\n",
        "            probs_c = self.density_function(\n",
        "                X, self.classes_mean[str(c)], self.classes_variance[str(c)]\n",
        "            )\n",
        "            probs[:, c] = probs_c + np.log(prior)\n",
        "\n",
        "        return np.argmax(probs, 1)\n",
        "\n",
        "    def density_function(self, x, mean, sigma):\n",
        "        # Calculate probability from Gaussian density function\n",
        "        const = -self.num_features / 2 * np.log(2 * np.pi) - 0.5 * np.sum(\n",
        "            np.log(sigma + self.eps)\n",
        "        )\n",
        "        probs = 0.5 * np.sum(np.power(x - mean, 2) / (sigma + self.eps), 1)\n",
        "        return const - probs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-PJWFFcUI3C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test.drop(X_test.tail(-1).index, inplace=True)\n",
        "y_test.drop(X_test.tail(-1).index, inplace=True)\n",
        "nb = NaiveBayes(X_train, y_train)\n",
        "nb.fit(X_train, y_train)\n",
        "y_pred = nb.predict(X_test)\n",
        "accuracy_score(y_test, y_pred=y_pred)\n",
        "# Problem here is that test must be the same shape...."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0e6xpuCEUZGp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NaiveBayes:\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self._classes = np.unique(y)\n",
        "        n_classes = len(self._classes)\n",
        "\n",
        "        # calculate mean, var, and prior for each class\n",
        "        self._mean = np.zeros((n_classes, n_features), dtype=np.float64)\n",
        "        self._var = np.zeros((n_classes, n_features), dtype=np.float64)\n",
        "        self._priors =  np.zeros(n_classes, dtype=np.float64)\n",
        "\n",
        "        for idx, c in enumerate(self._classes):\n",
        "            X_c = X[y==c]\n",
        "            self._mean[idx, :] = X_c.mean(axis=0)\n",
        "            self._var[idx, :] = X_c.var(axis=0)\n",
        "            self._priors[idx] = X_c.shape[0] / float(n_samples)\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = [self._predict(x) for x in X]\n",
        "        return np.array(y_pred)\n",
        "\n",
        "    def _predict(self, x):\n",
        "        posteriors = []\n",
        "\n",
        "        # calculate posterior probability for each class\n",
        "        for idx, c in enumerate(self._classes):\n",
        "            prior = np.log(self._priors[idx])\n",
        "            posterior = np.sum(np.log(self._pdf(idx, x)))\n",
        "            posterior = prior + posterior\n",
        "            posteriors.append(posterior)\n",
        "            \n",
        "        # return class with highest posterior probability\n",
        "        return self._classes[np.argmax(posteriors)]\n",
        "            \n",
        "\n",
        "    def _pdf(self, class_idx, x):\n",
        "        mean = self._mean[class_idx]\n",
        "        var = self._var[class_idx]\n",
        "        numerator = np.exp(- (x-mean)**2 / (2 * var))\n",
        "        denominator = np.sqrt(2 * np.pi * var)\n",
        "        return numerator / denominator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egAQSW37iMJG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nb = NaiveBayes()\n",
        "nb.fit(X_train, y_train)\n",
        "y_pred = nb.predict(X_test)\n",
        "accuracy_score(y_test, y_pred=y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwRLI8RRA3yu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiNayes:\n",
        "    \"\"\"\n",
        "    Multinomial Naive Bayes algorithm.\n",
        "    Paramaters\n",
        "    ----------\n",
        "    alpha : float, default=1.0\n",
        "        Smoothing paramater, can be set to smaller values\n",
        "        (0 for no smoothing)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, alpha=1.0):\n",
        "        self.alpha = alpha\n",
        "        self.fitted = False\n",
        "\n",
        "    def label_binarizer(self, y, classes=None, bin_labels=None):\n",
        "        \"\"\"convert labels into an array of shape\n",
        "           (length of y, number of classes). This\n",
        "           will assist in getting the log priors and probabilities\"\"\"\n",
        "        if classes is None:\n",
        "            classes = np.unique(y)\n",
        "            bin_labels = np.zeros((y.shape[0], classes.shape[0]))\n",
        "            self.classes = classes\n",
        "            self.bin_labels = bin_labels\n",
        "\n",
        "        if bin_labels.shape[0] < 1:\n",
        "            return None\n",
        "\n",
        "        x = np.where(classes == y[0])\n",
        "        bin_labels[0][x] = 1\n",
        "\n",
        "        return self.label_binarizer(y[1:], classes, bin_labels[1:])\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # if X is not np.ndarray, convert from csr with `toarray()`\n",
        "        if type(X) is not np.ndarray:\n",
        "            X = X.to_numpy()\n",
        "\n",
        "        self.label_binarizer(y)\n",
        "\n",
        "        n_classes = self.classes.shape[0]\n",
        "        n_features = X.shape[1]\n",
        "\n",
        "        # initialize counter arrays\n",
        "        self.class_count = np.zeros(n_classes)\n",
        "        self.feature_count = np.zeros((n_classes, n_features))\n",
        "\n",
        "        # count classes and features by getting\n",
        "        # dot product of transposed binary labels\n",
        "        # they are automatically separated into their\n",
        "        # appropriate arrays\n",
        "        self.feature_count += np.dot(self.bin_labels.T, X)\n",
        "        self.class_count += self.bin_labels.sum(axis=0)\n",
        "\n",
        "        # add smoothing\n",
        "        if self.alpha > 0.0:\n",
        "            self.feature_count += self.alpha\n",
        "            smoothed_class_count = self.feature_count.sum(axis=1)\n",
        "\n",
        "            # get conditional log probabilities\n",
        "            self.feat_log_probs = (np.log(self.feature_count) -\n",
        "                                   np.log(smoothed_class_count.reshape(-1, 1)))\n",
        "        else:\n",
        "            print(\n",
        "                f\"Alpha is {self.alpha}. A value this small will cause \"\n",
        "                \"result in errors when feature count is 0\"\n",
        "            )\n",
        "            self.feat_log_probs = np.log(\n",
        "                                    self.feature_count /\n",
        "                                    self.feature_count\n",
        "                                    .sum(axis=1)\n",
        "                                    .reshape(-1, 1)\n",
        "                                  )\n",
        "\n",
        "        # get log priors\n",
        "        self.class_log_priors = (np.log(self.class_count) -\n",
        "                                 np.log(self.class_count\n",
        "                                 .sum(axis=0)\n",
        "                                 .reshape(-1, 1)))\n",
        "\n",
        "        self.fitted = True\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict target from features of X\"\"\"\n",
        "\n",
        "        # check if model has fit data\n",
        "        if not self.fitted:\n",
        "            print(\"The classifier has not yet \"\n",
        "                  \"been fit. Not executing predict\")\n",
        "\n",
        "        if type(X) is not np.ndarray:\n",
        "            X = X.toarray()\n",
        "\n",
        "        scores = np.dot(X, self.feat_log_probs.T) + self.class_log_priors\n",
        "\n",
        "        predictions = self.classes[np.argmax(scores, axis=1)]\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def accuracy(self, y_pred, y):\n",
        "        points = (y_pred == y).astype(int)\n",
        "        score = points.sum() / points.shape[0]\n",
        "\n",
        "        return score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHglF0lqA55F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf = MultiNayes()\n",
        "\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy_score(y_test, y_pred=y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSOvJyPj_USb",
        "colab_type": "text"
      },
      "source": [
        "## Bernoulli"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOGeNoHMc9zI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(text):\n",
        "  text = text.lower() # convert to lowercase\n",
        "  stop_chars = '''0123456789!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
        "\n",
        "\n",
        "  # To take input from the user\n",
        "  # my_str = input(\"Enter a string: \")\n",
        "\n",
        "  # remove punctuation from the string\n",
        "  tokens = \"\"\n",
        "  for char in text:\n",
        "    if char not in stop_chars:\n",
        "        tokens += char\n",
        "\n",
        "  return tokens.split()\n",
        "\n",
        "def count_words(training_set):\n",
        "  \"\"\" \n",
        "  input is (message, category) where the category is 0 or 1\n",
        "  output is counts = {word1: [num_cat1, num_cat2], word2: [num_cat1, num_cat2]...}\n",
        "  \"\"\"\n",
        "  counts = {}\n",
        "  for message, category in training_set:\n",
        "    for word in tokenize(message):\n",
        "      if word not in counts:\n",
        "        counts[word] = [0,0]\n",
        "      counts[word][category] += 1\n",
        "  return counts\n",
        "\n",
        "def word_probabilities(counts, total_cat_1, total_cat_2, k=0.5):\n",
        "  \"\"\"turn the word_counts into a list of triplets\n",
        "  w, p(w | cat1) and p(w | cat2)\"\"\"\n",
        "  word, \n",
        "  return [(w,\n",
        "  (spam + k) / (total_spams + 2 * k),\n",
        "  (non_spam + k) / (total_non_spams + 2 * k))\n",
        "  for w, (spam, non_spam) in counts.items()]\n",
        "\n",
        "def spam_probability(word_probs, message):\n",
        "  message_words = tokenize(message)\n",
        "  log_prob_if_spam = log_prob_if_not_spam = 0.0\n",
        "  # iterate through each word in our vocabulary\n",
        "  for word, prob_if_spam, prob_if_not_spam in word_probs:\n",
        "    # if *word* appears in the message,\n",
        "    # add the log probability of seeing it\n",
        "    if word in message_words:\n",
        "      log_prob_if_spam += math.log(prob_if_spam)\n",
        "      log_prob_if_not_spam += math.log(prob_if_not_spam)\n",
        "    # if *word* doesn't appear in the message\n",
        "  # add the log probability of _not_ seeing it\n",
        "  # which is log(1 - probability of seeing it)\n",
        "    else:\n",
        "      log_prob_if_spam += math.log(1.0 - prob_if_spam)\n",
        "      log_prob_if_not_spam += math.log(1.0 - prob_if_not_spam)\n",
        "\n",
        "  prob_if_spam = math.exp(log_prob_if_spam)\n",
        "  prob_if_not_spam = math.exp(log_prob_if_not_spam)\n",
        "  return prob_if_spam / (prob_if_spam + prob_if_not_spam)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdjsGvW42nte",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NaiveBayesClassifier:\n",
        "  def __init__(self, k=0.5):\n",
        "    self.k = k\n",
        "    self.word_probs = []\n",
        "  def train(self, training_set):\n",
        "    # count spam and non-spam messages\n",
        "    num_spams = len([is_spam\n",
        "    for message, is_spam in training_set\n",
        "    if is_spam])\n",
        "    num_non_spams = len(training_set) - num_spams\n",
        "    # run training data through our \"pipeline\"\n",
        "    word_counts = count_words(training_set)\n",
        "    self.word_probs = word_probabilities(word_counts,\n",
        "    num_spams,\n",
        "    num_non_spams,\n",
        "    self.k)\n",
        "  def classify(self, message):\n",
        "    return spam_probability(self.word_probs, message)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZkPMdBP3F8W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nbc = NaiveBayesClassifier()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbDjlTKCFwJJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Let's say input is [(\"sample text1\", label1), (\"sample text2\", label2)]\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TI_GCxkbNe_Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "5af4921b-3312-40fe-8cb6-6d5e6ba59801"
      },
      "source": [
        "text = \"will this work? The rights among men can be strenuous...\"\n",
        "punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
        "\n",
        "\n",
        "# To take input from the user\n",
        "# my_str = input(\"Enter a string: \")\n",
        "\n",
        "# remove punctuation from the string\n",
        "no_punct = \"\"\n",
        "for char in text:\n",
        "   if char not in punctuations:\n",
        "       no_punct = no_punct + char\n",
        "no_punct.split()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['will',\n",
              " 'this',\n",
              " 'work',\n",
              " 'The',\n",
              " 'rights',\n",
              " 'among',\n",
              " 'men',\n",
              " 'can',\n",
              " 'be',\n",
              " 'strenuous']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9Rt3ImXu7yz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize1(text):\n",
        "  text = text.lower() # convert to lowercase\n",
        "  stop_chars = '''0123456789!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
        "\n",
        "\n",
        "  # To take input from the user\n",
        "  # my_str = input(\"Enter a string: \")\n",
        "\n",
        "  # remove punctuation from the string\n",
        "  tokens = \"\"\n",
        "  for char in text:\n",
        "    if char not in stop_chars:\n",
        "        tokens += char\n",
        "\n",
        "  return tokens.split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLgMtsG6xAw4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def count_words(training_set):\n",
        "  \"\"\" \n",
        "  input is (message, category) where the category is 0 or 1\n",
        "  output is counts = {word1: [num_cat1, num_cat2], word2: [num_cat1, num_cat2]...}\n",
        "  \"\"\"\n",
        "  counts = {}\n",
        "  for message, category in training_set:\n",
        "    for word in tokenize1(message):\n",
        "      if word not in counts:\n",
        "        counts[word] = [0,0]\n",
        "      counts[word][category] += 1\n",
        "  return counts\n",
        "\n",
        "def word_probabilities(counts, total_cat_1, total_cat_2, k=0.5):\n",
        "  \"\"\"turn the word_counts into a list of triplets\n",
        "  w, p(w | cat1) and p(w | cat2)\"\"\"\n",
        "\n",
        "  return [(w,\n",
        "  (cat1 + k) / (total_cat_1 + 2 * k),\n",
        "  (total_cat_2 + k) / (total_cat_2 + 2 * k))\n",
        "  for w, (cat1, cat2) in counts.items()]\n",
        "\n",
        "def cat_probability(word_probs, message):\n",
        "  message_words = tokenize1(message)\n",
        "  log_prob_cat_1 = log_prob_cat_2 = 0.0\n",
        "  # iterate through each word in our vocabulary\n",
        "  for word, prob_cat_1, prob_cat_2 in word_probs:\n",
        "    # if *word* appears in the message,\n",
        "    # add the log probability of seeing it\n",
        "    if word in message_words:\n",
        "      log_prob_cat_1 += np.log(prob_cat_1)\n",
        "      log_prob_cat_2 += np.log(prob_cat_2)\n",
        "    # if *word* doesn't appear in the message\n",
        "  # add the log probability of _not_ seeing it\n",
        "  # which is log(1 - probability of seeing it)\n",
        "    else:\n",
        "      log_prob_cat_1 += np.log(1.0 - prob_cat_1)\n",
        "      log_prob_cat_2 += np.log(1.0 - prob_cat_2)\n",
        "\n",
        "  prob_cat_1 = np.exp(log_prob_cat_1)\n",
        "  prob_cat_2 = np.exp(log_prob_cat_2)\n",
        "  return prob_cat_1 / (prob_cat_1 + prob_cat_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hATo4-Mv-E-9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d2424a74-9f57-4b13-def5-e63ecf7c0e86"
      },
      "source": [
        "trainin = [\n",
        "           (\"\"\"Our second function will count the words in a labeled training set of messages. We’ll have\n",
        "it return a dictionary whose keys are words, and whose values are two-element lists\n",
        "[spam_count, non_spam_count] corresponding to how many times we saw that word in\n",
        "both spam and nonspam messages:\n",
        "\"\"\", 0),\n",
        "(\"\"\"Our next step is to turn these counts into estimated probabilities using the smoothing we\n",
        "described before. Our function will return a list of triplets containing each word, the\n",
        "probability of seeing that word in a spam message, and the probability of seeing that word\n",
        "in a nonspam message:\"\"\", 1),\n",
        "(\"\"\"The last piece is to use these word probabilities (and our Naive Bayes assumptions) to\n",
        "assign probabilities to messages\"\"\", 0)\n",
        "]\n",
        "# count spam and non-spam messages\n",
        "num_cat_1 = len([cat\n",
        "for message, cat in trainin\n",
        "if cat==0])\n",
        "num_cat_2 = len(trainin) - num_cat_1\n",
        "word_counts = count_words(trainin)\n",
        "\n",
        "word_probs = word_probabilities(word_counts, num_cat_1, num_cat_2)\n",
        "message = \"The last piece is to use these word probabilities\"\n",
        "cat_probability(word_probs, message)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeiwyMMH_OC3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "886e120f-5470-4e34-c71a-45dc30641ea8"
      },
      "source": [
        "count_words(trainin)\n",
        "word_probs\n",
        "# here we have the word and the probability. \n",
        "# remember we have to switch to counts of words instead of binary\n",
        "# TODO multiply a prior probability of each category by the probability of each \n",
        "# word in the message given it was in each category. p(cat1) * p(w|cat1) * p(w1|cat1)\n",
        "# etc. same for cat2\n",
        "# then compare and predict the higher value."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('twoelement', 0.5, 0.75),\n",
              " ('count', 0.5, 0.75),\n",
              " ('that', 0.5, 0.75),\n",
              " ('corresponding', 0.5, 0.75),\n",
              " ('many', 0.5, 0.75),\n",
              " ('we', 0.5, 0.75),\n",
              " ('values', 0.5, 0.75),\n",
              " ('saw', 0.5, 0.75),\n",
              " ('it', 0.5, 0.75),\n",
              " ('of', 0.5, 0.75),\n",
              " ('function', 0.5, 0.75),\n",
              " ('our', 0.8333333333333334, 0.75),\n",
              " ('messages', 0.8333333333333334, 0.75),\n",
              " ('keys', 0.5, 0.75),\n",
              " ('a', 0.5, 0.75),\n",
              " ('times', 0.5, 0.75),\n",
              " ('and', 0.8333333333333334, 0.75),\n",
              " ('have', 0.5, 0.75),\n",
              " ('in', 0.5, 0.75),\n",
              " ('training', 0.5, 0.75),\n",
              " ('set', 0.5, 0.75),\n",
              " ('spam', 0.5, 0.75),\n",
              " ('second', 0.5, 0.75),\n",
              " ('dictionary', 0.5, 0.75),\n",
              " ('word', 0.8333333333333334, 0.75),\n",
              " ('whose', 0.5, 0.75),\n",
              " ('labeled', 0.5, 0.75),\n",
              " ('the', 0.8333333333333334, 0.75),\n",
              " ('return', 0.5, 0.75),\n",
              " ('are', 0.5, 0.75),\n",
              " ('both', 0.5, 0.75),\n",
              " ('lists', 0.5, 0.75),\n",
              " ('will', 0.5, 0.75),\n",
              " ('words', 0.5, 0.75),\n",
              " ('how', 0.5, 0.75),\n",
              " ('nonspam', 0.5, 0.75),\n",
              " ('nonspamcount', 0.5, 0.75),\n",
              " ('to', 0.8333333333333334, 0.75),\n",
              " ('spamcount', 0.5, 0.75),\n",
              " ('we’ll', 0.5, 0.75),\n",
              " ('containing', 0.16666666666666666, 0.75),\n",
              " ('triplets', 0.16666666666666666, 0.75),\n",
              " ('described', 0.16666666666666666, 0.75),\n",
              " ('these', 0.5, 0.75),\n",
              " ('using', 0.16666666666666666, 0.75),\n",
              " ('seeing', 0.16666666666666666, 0.75),\n",
              " ('probabilities', 0.5, 0.75),\n",
              " ('estimated', 0.16666666666666666, 0.75),\n",
              " ('step', 0.16666666666666666, 0.75),\n",
              " ('counts', 0.16666666666666666, 0.75),\n",
              " ('message', 0.16666666666666666, 0.75),\n",
              " ('is', 0.5, 0.75),\n",
              " ('each', 0.16666666666666666, 0.75),\n",
              " ('into', 0.16666666666666666, 0.75),\n",
              " ('smoothing', 0.16666666666666666, 0.75),\n",
              " ('probability', 0.16666666666666666, 0.75),\n",
              " ('next', 0.16666666666666666, 0.75),\n",
              " ('list', 0.16666666666666666, 0.75),\n",
              " ('turn', 0.16666666666666666, 0.75),\n",
              " ('before', 0.16666666666666666, 0.75),\n",
              " ('naive', 0.5, 0.75),\n",
              " ('use', 0.5, 0.75),\n",
              " ('piece', 0.5, 0.75),\n",
              " ('assumptions', 0.5, 0.75),\n",
              " ('assign', 0.5, 0.75),\n",
              " ('bayes', 0.5, 0.75),\n",
              " ('last', 0.5, 0.75)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    }
  ]
}